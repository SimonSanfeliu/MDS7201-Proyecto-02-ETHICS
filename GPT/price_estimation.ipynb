{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de paquetes necesarios en caso de no tenerlos\n",
    "\n",
    "%%capture\n",
    "!pip install tiktoken\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias\n",
    "import tiktoken as tiktoken\n",
    "import json\n",
    "import requests\n",
    "from math import ceil\n",
    "import openai\n",
    "openai.api_key = \"agregar key de OpenAI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas con GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En la primera instancia individual, el estudiante valora la opción de no usar la información del grupo de WhatsApp con un 6, argumentando que afectaría su futuro. En la instancia grupal, el estudiante valora la misma opción con un 4, argumentando que es un punto medio entre copiar y no copiar, y que quizás copiar es la única opción que le queda si su beca está en riesgo. En la segunda instancia individual, el estudiante valora la opción con un 5, argumentando que su posición cambió un poco y que sigue creyendo que copiar es una opción si su beca está en riesgo. En el chat grupal, se discute sobre las ventajas y desventajas de copiar, la importancia de la beca y las consecuencias de copiar.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat a analizar\n",
    "import pandas as pd\n",
    "id_user = 3200\n",
    "\n",
    "data = pd.read_csv(\"clean_julieta.csv\")\n",
    "Ind1 = data[data['user_id'] == id_user][['opt_left', 'opt_right', 'sel', 'comment', 'etapa']].iloc[0]\n",
    "Grup = data[data['user_id'] == id_user][['opt_left', 'opt_right', 'sel', 'comment', 'etapa']].iloc[1]\n",
    "Ind2 = data[data['user_id'] == id_user][['opt_left', 'opt_right', 'sel', 'comment', 'etapa']].iloc[2]\n",
    "\n",
    "text_file = open(\"chat_julieta/team_1546.0_chat.txt\", \"r\")\n",
    "data = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "prompt = f\"\"\"Se realizó un estudio en un grupo de estudiantes en el que se les presentó un dilema ético \n",
    "y las dos opciones entre las que pueden decidir. Los estudiantes participaron de 3 instancias: una individual, \n",
    "una grupal y finalmente otra individual. El objetivo de los estudiantes es manifestar su postura entre ambas \n",
    "opciones usando valores entre 1 y 6. Se te proporciona \n",
    "la información de cada una de las etapas en las que participaron los estudiantes, es decir, en la primera etapa \n",
    "individual tendrás la valorización y justificación del estudiante, en la etapa grupal tendrás un chat de grupo \n",
    "en el que los estudiantes discuten y llegan a un acuerdo de valorización en conjunto junto con su justificación, \n",
    "y finalmente en la última etapa individual vuelves a tener la valorización y justificación del estudiante tras \n",
    "haber participado de la instancia grupal. Tus tareas son: primero, identificar en las justificaciones textuales los \n",
    "elementos principales que sostienen la postura (de 1 a 6) del estudiante en cada instancia individual;\n",
    "luego identificar los elementos que sostienen la postura grupal usando los registros del chat, después identificar \n",
    "aquellos elementos que cambiaron en las justificaciones individuales entre la primera y segunda instancia,\n",
    "identificando qué elementos del chat grupal están relacionados con la justificación de la segunda instancia.\n",
    "Todo esto debe ser entregado en una respuesta breve, no más de 100 palabras.\n",
    "La valorización y justificación de la instancia individual 1 son: {Ind1}\n",
    "La valorización y justificación de la instancia grupal son: {Grup}\n",
    "La valorización y justificación de la instancia individual 2 son: {Ind2}\n",
    "El chat grupal se proporciona a continuación: {data}\"\"\"\n",
    "\n",
    "# Input para la API\n",
    "messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"name\": \"user\",\n",
    "     \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Modelo a usar\n",
    "model = \"gpt-3.5-turbo-16k-0613\"\n",
    "\n",
    "# Output tokens\n",
    "max_tokens=1000\n",
    "\n",
    "# Respuesta de la API\n",
    "response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "# Output de la API\n",
    "output_tokens = response.usage.completion_tokens\n",
    "output = response.choices[0].message.content\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función oficial de OpeanAI para obtener el número de tokens\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API gratuita para obtener el precio actualizado del dólar a pesos chilenos\n",
    "\n",
    "class Mindicador:\n",
    "    def __init__(self, indicador):\n",
    "        self.indicador = indicador\n",
    "\n",
    "\n",
    "    def InfoApi(self):\n",
    "        # En este caso hacemos la solicitud para el caso de consulta de un indicador en un año determinado\n",
    "        url = f'https://mindicador.cl/api/{self.indicador}'\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text.encode(\"utf-8\"))\n",
    "        # Para que el json se vea ordenado, retornar pretty_json\n",
    "        pretty_json = json.dumps(data, indent=2)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener el precio total de un prompt\n",
    "\n",
    "def tokens(text, model, output_tokens):\n",
    "    messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"name\": \"user\",\n",
    "             \"content\": text\n",
    "             }\n",
    "        ]\n",
    "\n",
    "    # Number of tokens\n",
    "    n_tokens = num_tokens_from_messages(messages, model)\n",
    "\n",
    "    # Dollar to chilean pesos\n",
    "    indicador = Mindicador(\"dolar\")\n",
    "    dolar = indicador.InfoApi()[\"serie\"][0][\"valor\"]\n",
    "\n",
    "    # Dictionaries with respective prices\n",
    "    input_dict = {\"gpt-3.5-turbo-0613\": 0.0015, \"gpt-3.5-turbo-16k-0613\": 0.003, \"gpt-4-0314\": 0.03, \"gpt-4-32k-0314\":\n",
    "        0.06, \"gpt-4-0613\": 0.03, \"gpt-4-32k-0613\": 0.06}\n",
    "    output_dict = {\"gpt-3.5-turbo-0613\": 0.002, \"gpt-3.5-turbo-16k-0613\": 0.004, \"gpt-4-0314\": 0.06, \"gpt-4-32k-0314\":\n",
    "        0.12, \"gpt-4-0613\": 0.06, \"gpt-4-32k-0613\": 0.12}\n",
    "    \n",
    "    input_price = ceil(input_dict[model] * n_tokens // 1000)\n",
    "    input_precio = ceil(input_dict[model] * dolar * n_tokens // 1000)\n",
    "    output_price = ceil(output_dict[model] * output_tokens // 1000)\n",
    "    output_precio = ceil(output_dict[model] * dolar * output_tokens // 1000)\n",
    "    resp_dic = {\"model\": model,\n",
    "                \"numTokens\": n_tokens,\n",
    "                \"input price [USD]\": input_price,\n",
    "                \"output price [USD]\": output_price,\n",
    "                \"total price [USD]\": input_price+output_price,\n",
    "                \"input precio [CLP]\": input_precio,\n",
    "                \"output precio [CLP]\": output_precio,\n",
    "                \"total precio [CLP]\": input_precio+output_precio}\n",
    "    return resp_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo-16k-0613',\n",
       " 'numTokens': 3276,\n",
       " 'input price [USD]': 0,\n",
       " 'output price [USD]': 0,\n",
       " 'total price [USD]': 0,\n",
       " 'input precio [CLP]': 8,\n",
       " 'output precio [CLP]': 0,\n",
       " 'total precio [CLP]': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens(prompt, model, output_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
